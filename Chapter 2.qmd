---
title: "Chapter 2 - Working with Text Data"
format: html
execute:
    error: true
---

# 2 - Working with Text Data

## 2.1 Understanding word embeddings

We need to understand that LLMs cannot process raw text directly and instead need to be represented with continuous-valued vectors. The conversion is done through embeddings which take in the raw data, in the form of video, audio, text, etc. and converts them to a dense vector of continous values, which are then used in deep learning.

## 2.2 Tokenizing text

Before creating embeddings however, text data requires a preprocessing step to create tokens which is how we split input text up into pieces for an embedding layer. To summarize, we split an input text into individual tokens, which are either words or special characters, such as punctuation characters. These are later converted into token IDs (like a lookup table) which are then used to create embeddings.

```{python}
with open('the_verdict.txt', 'r',encoding='utf_8') as f:
    raw_text = f.read()

print('Total number of characters:', len(raw_text))
print(raw_text[:99])
```

Goal is to tokenize this 20479 character short story.

Then turn into embeddings for LLM training.

So use regex to get a list of individual words, whitespaces, and punctuation characters

```{python}
import re

text = 'Hello, world. This, is a test.'
result = re.split(r'(\s)',text)
print(result)
```

Now split on whitespaces and commas (\s), and periods (\[,.\])

```{python}
result = re.split(r'([,.]|\s)',text)
print(result)
```

We could remove whitespaces so we just have words, commas, and periods

```{python}
result = [item for item in result if item.strip()]
print(result)
```

::: callout-note
Potential experiment is to keep white spaces in the tokenizer and see the differences in model output. Think about how picky coding languages are when it comes to spacing!
:::

So now modify the tokenization scheme to work on other types of punction, such as question marks, quotation marks, and the double dashes shown in the first 100 characters.

```{python}
text = 'Hello, world. Is this-- a test?'
result = re.split(r'([,.:;?_!"()\']|--|\s)',text)
result = [item.strip() for item in result if item.strip()]
print(result)
```

Now apply to the entire short story

```{python}
preprocessed = re.split(r'([,.:;?_!"()\']|--|\s)',raw_text)
preprocessed = [item.strip() for item in preprocessed if item.strip()]
print(len(preprocessed))
print(preprocessed[:30])
```

## 2.3 Converting tokens into token IDs

We now take these tokens which are a Python string and convert them to an integer representation to produce token IDs. An intermediate step before going to embedding vectors. This mapping from tokens to token IDs requires a \`\`vocabulary''.

```{python}
all_words = sorted(set(preprocessed)) # Get all unique words
vocab_size = len(all_words)
print(vocab_size)
```

Let's show some of this lookup, it'll be a perfect use of a dictionary data type.

```{python}
vocab = {token:integer for integer, token in enumerate(all_words)}
for i, item in enumerate(vocab.items()):
    print(item)
    if i > 50:
        break
```

::: callout-tip
We will use an inverse of this to go from our token IDs back to the original words!
:::

Let's create a complete tokenizer class which can go both directions, `encode` method (string-to-integer) and `decode` (integer-to-string) method.

```{python}
class SimpleTokenizerV1:
    def __init__(self, vocab):
        # Store the vocab as a class attribute for access in the encode and decode methods
        self.str_to_int = vocab
        # Create an inverse vocabularly that maps token IDs back to the original text tokens
        self.int_to_str = {i:s for s, i in vocab.items()}

    # Process input text into token IDs
    def encode(self, text):
        preprocessed = re.split(r'([,.:;?_!"()\']|--|\s)',text) # Seperate words/punctution
        preprocessed = [item.strip() for item in preprocessed if item.strip()] # Remove white space
        ids = [self.str_to_int[s] for s in preprocessed] # Input string and get key from vocab dict
        return ids

    # Convert token IDs back to text
    def decode(self, ids):
        text = " ".join([self.int_to_str[i] for i in ids])
        # Replace spaces before the specified punctuation
        text = re.sub(r'\s+([,.?!"()\'])', r'\1', text)
        return text

```

-   `__init__:` This is the constructor method that initializes the class. It takes vocab as an argument.
    -   `self.str_to_int:` Stores the vocabulary dictionary where keys are strings (tokens) and values are integers (token IDs).
    -   `self.int_to_str:` Creates an inverse dictionary where keys are token IDs and values are the original strings (tokens).
-   `encode:` This method processes input text into token IDs.
    -   `re.split(r'([,.:;?_!"()\']|--|\s)', text):` Splits the text into words and punctuation based on the specified regular expression pattern.
    -   `[item.strip() for item in preprocessed if item.strip()]:` Strips whitespace from each item and removes empty strings.
    -   `[self.str_to_int[s] for s in preprocessed]:` Converts each token in the preprocessed list to its corresponding token ID using the str_to_int dictionary.
    -   `return ids:` Returns the list of token IDs.
-   `decode:` This method converts token IDs back to text.
    -   `" ".join([self.int_to_str[i] for i in ids]):` Joins the tokens corresponding to the token IDs into a single string with spaces in between.
    -   `re.sub(r'\s+([,.:;?_!"()\'])', r'\1', text):` Removes spaces before punctuation marks.
    -   `return text:` Returns the reconstructed text.

### Example

```{python}
vocab = {'hello': 1, 'world': 2, ',': 3}
tokenizer = SimpleTokenizerV1(vocab)

encoded = tokenizer.encode("hello, world")
print(encoded)  # Output: [1, 3, 2]

decoded = tokenizer.decode(encoded)
print(decoded)  # Output: "hello, world"
```

Back to the short story vocab

```{python}
vocab = {token:integer for integer, token in enumerate(all_words)}
tokenizer = SimpleTokenizerV1(vocab)

text = """"It's the last he painted, you know," Mrs. Gisburn said with pardonable pride."""

ids = tokenizer.encode(text)
print(ids)
```

The code above prints the following token IDs followed by the code below for the decoding

```{python}
print(tokenizer.decode(ids))
```

We can apply it to other text as well but be careful if any words are not in our vocab. This will cause an error!

```{python}
# text = 'Hello, do you like tea?'
# print(tokenizer.encode(text))
```

## 2.4 Adding special context tokens

This is how you will handle unknown words, but also to identify the end of the text.

```{python}
all_tokens = sorted(list(set(preprocessed)))
all_tokens.extend(['<|endoftext|>','<|unk|>']) # Add to vocab
vocab = {token:integer for integer, token in enumerate(all_tokens)}

print(len(vocab.items()))
```

The new vocab has increased by two!

```{python}
for i, item in enumerate(list(vocab.items())[-5:]):
    print(item)
```

Adjusting the tokenizer class from before

```{python}
class SimpleTokenizerV2:
    def __init__(self, vocab):
        self.str_to_int = vocab
        self.int_to_str = {i:s for s, i in vocab.items()}

    # Process input text into token IDs
    def encode(self, text):
        preprocessed = re.split(r'([,.:;?_!"()\']|--|\s)',text) # Seperate words/punctution
        preprocessed = [item.strip() for item in preprocessed if item.strip()] # Remove white space
        # Replace unknown words with <|unk|> tokens
        preprocessed = [item if item in self.str_to_int else "<|unk|>" for item in preprocessed] 

        ids = [self.str_to_int[s] for s in preprocessed] # Input string and get key from vocab dict
        return ids

    # Convert token IDs back to text
    def decode(self, ids):
        text = " ".join([self.int_to_str[i] for i in ids])
        # Replace spaces before the specified punctuation
        text = re.sub(r'\s+([,.?_!"()\'])', r'\1', text)
        return text
```

Now let's try the new tokenzier out on two independent and unrelated sentances that are concacted togther.

```{python}
text1 = 'Hello, do you like tea?'
text2 = 'In the sunlit terraces of the palace.'
text = ' <|endoftext|> '.join([text1,text2])
print(text)
```

Let's tokenize!

```{python}
tokenizer = SimpleTokenizerV2(vocab)
print(tokenizer.encode(text))
```

And detokenize to check!

```{python}
print(tokenizer.decode(tokenizer.encode(text)))
```

Depending on the LLM, some researchers implement additional special tokens such as:

-   `[BOS]` *(beginning of sequence)*-- This token marks the start of a text.

-   `[EOS]` *(end of sequence)*-- This token is positioned at the end of a text and is especially useful when concatenating multiple unrelated texts, similar to `<|endoftext|>`

-   `[PAD]` *(padding)*-- When training LLMs with batch sizes larger than one, the batch might contain texts of varying lengths. To ensure all texts have the same length, the shorter texts are extended or "padded": using the *`[PAD]`* token, up to the length of the longest text in the batch.

The tokenizer used for the GPT models does not need any of these tokens; it only uses an `<|endoftext|>` token for simplicity. `<|endoftext|>` is analogous to the `[EOS]` token. `<|endoftext|>` is also useful for padding. However, when training on batched inputs, we typically use a mask, meaning we don't attend to padded tokens. Therefore the specific token chosen for padding becomes inconsequential.

Moreover, the tokenizer used for GPT models also doesn't use an `<|unk|>` token for out-of-vocabularly words. Instead, GPT models use a *byte pair encoding* tokenizer, which breaks words down into subword units. 

## 2.5 Byte pair encoding (BPE)
BPE tokenizer was used in GPT-2 and GPT-3. Since it's a bit complicated, we will use an open source library called [tiktoken](https://github.com/openai/tiktoken) which implements the BPE algorithm efficienctly in Rust.

```{python}
from importlib.metadata import version
import tiktoken
print('tiktoken version:', version('tiktoken'))
```

Once installed, we can instatiate the BPE tokenizer from tiktoken as follows:

```{python}
tokenizer = tiktoken.get_encoding('gpt2')
```

The usage of this tokenizer is very similar to the `SimpleTokenizerV2` we implemented previously with an `encode` method:

```{python}
text = (
    "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.")
integers = tokenizer.encode(text,allowed_special = {'<|endoftext|>'})
print(integers)
```

Which we can convert back to text with a `decode` method:

```{python}
strings = tokenizer.decode(integers)
print(strings)
```

Two things about this. First, `<|endoftext|>` is a relatively large token ID at `50256`. The BPE tokenizer actually has a total vocabularly size of 50,257, with `<|endoftext|>` being the largest token ID.

Second, the BPE tokenizer encodes and decodes unknown words, such as `someunknownPlace` correctly. The BPE tokenizer can handle any uknown word. The algorithm underlying BPE breaks down words that aren't in its predefined vocabularly into smaller subword units or even individual characters, enabling it to handle out-of-vocabularly words. Which means it can process any text, even if it contains words that were not present in the training data. 

::: {.callout-note collapse="true"}
## Exercise 2.1 Byte pair encoding of unkown words

Try the BPE tokenizer from the tiktoken library on the unknown words "Akwirw ier" and print the individual token IDs. Then, call the decode function on each of the resulting  integers in this list. Lastly, call the  decode method on the token IDs to check whether it can reconstruct the original input, "Akwirw ier"

```{python}
print(tokenizer.encode('Akwirw ier'))
for individual_token in tokenizer.encode('Akwirw ier'):
    print(tokenizer.decode_single_token_bytes(individual_token))
    # b in front of strings indicates that the strings are byte strings.
print(tokenizer.decode(tokenizer.encode('Akwirw ier')))
```

:::

## 2.6 Data sampling with a sliding window

Let's implement a data loader that fetches the input-target pairs from the training dataset using a sliding window approach. Basically since the next word is always being predicted the window includes all the text "it has seen", then receieves the next word for the prediction, then at the next iteration that predicted word is revealed and added to the input window.

```{python}
with open('the_verdict.txt','r',encoding='utf-8') as f:
    raw_text = f.read()

enc_text = tokenizer.encode(raw_text)
print(len(enc_text))
# Remove the first 50 tokens from the dataset for demostration purposes. 
enc_sample = enc_text[50:]
```

Easiest way to create the input-target pairs for the next-word prediction task is to create two variables. It mirrors traditional supervised learning of having a variable `x` and a response `y`.

```{python}
context_size = 4 # The context size determines how many tokens are in the input
x = enc_sample[:context_size]
y = enc_sample[1:context_size+1]
print(f'x: {x}')
print(f'y:      {y}')
```

So the samples provided to the model for how we are going to predict are: 

```{python}
for i in range(1, context_size+1):
    context = enc_sample[:i]
    desired = enc_sample[i]
    print(context,'---->',desired)
    print(tokenizer.decode(context),'---->',tokenizer.decode([desired]))
```
Ok so these two concepts need to be implemented into a PyTorch data loader so there needs to be an `x` tensor which is the length of the context size and the `y` tensor which are the targets (`x` shifted over 1) which are also the length of the context size. 

```{python}
import torch
from torch.utils.data import Dataset, DataLoader
# A dataset for batched inputs and targets
class GPTDatasetV1(Dataset):
    def __init__(self, txt, tokenizer, max_length, stride):
        self.input_ids = []
        self.target_ids = []

        token_ids = tokenizer.encode(txt)
        # Implement logic from earlier with x and y
        # Sliding window to chunk text into overlapping sequences of max length
        for i in range(0, len(token_ids) - max_length, stride):
            input_chunk = token_ids[i : i + max_length]
            target_chunk = token_ids[i + 1: i + max_length + 1]
            self.input_ids.append(torch.tensor(input_chunk))
            self.target_ids.append(torch.tensor(target_chunk))
    # Return total number of rows in dataset
    def __len__(self):
        return len(self.input_ids)
    # REturns a single row from teh dataset
    def __getitem__(self, idx):
        return self.input_ids[idx], self.target_ids[idx]
```

The `GPTDatasetV1` class is based on the PyTorch `Dataset` class and efines how indvidual rows are fetched from the dataset, where each row consists of a number of token IDs (based on `max_length`) assigned to an `input_chunk` tensor. The `target_chunk` tensor contains the corresponding targets. Now this will be implemented into a PyTorch dataloader.


```{python}
# A data loader to generate batches with input-target pairs
def create_dataloader_v1(txt, batch_size = 4, max_length = 256, stride= 128, shuffle=True, drop_last = True, num_workers = 0):
    # Initialize tokenizer
    tokenizer = tiktoken.get_encoding('gpt2')
    # Create dataset
    dataset = GPTDatasetV1(txt, tokenizer, max_length,stride)
    dataloader  = DataLoader(dataset, batch_size=batch_size,shuffle=shuffle,drop_last=drop_last,num_workers=num_workers)
    # drop_last=True drops the last batch if its shorter than the specified batch_size to prevent loss spikes during training
    # num_workers is the number of CPU processes to use for preprocessing
    return dataloader
```

Testing the `dataloader` with a batch size of 1 for an LLM with a context size of 4 to develop an intuition of how the `GPTDatasetV1` class from listing 2.5 and the `create_dataloader_v1` function work together.

```{python}
with open('the_verdict.txt','r',encoding='utf-8') as f:
    raw_text = f.read()

dataloader = create_dataloader_v1(
    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)
data_iter = iter(dataloader)
first_batch = next(data_iter)
print(first_batch)
```
`first_batch` variable is two tensors: the first stores the input token IDs and the second contains the target token IDs. Since the `max_length` is set of 4, each of the two tensors is length 4. Typically an LLM is trained on inputs of size 256 and up. 

The `stride` is how much each batch is shifted from each other.

```{python}
second_batch = next(data_iter)
print(second_batch)
```

A few examples to play with the dataloader function.

```{python}
dataloader = create_dataloader_v1(
    raw_text, batch_size=1, max_length=3, stride=2, shuffle=False)
data_iter = iter(dataloader)
first_batch = next(data_iter)
print(first_batch)
second_batch = next(data_iter)
print(second_batch)
```

::: {.callout-note collapse="true"}
## Exercise 2.2 Data loaders with different stides and context sizes

To develop more intution for how the data loader works, try to run it with differnt settings such as `max_length=2` and `stride=2`, and `max_length=8` and `stride=2`.

```{python}
dataloader = create_dataloader_v1(
    raw_text, batch_size=1, max_length=2, stride=2, shuffle=False)
data_iter = iter(dataloader)
print(next(data_iter))
print(next(data_iter))

dataloader = create_dataloader_v1(
    raw_text, batch_size=1, max_length=8, stride=2, shuffle=False)
data_iter = iter(dataloader)
print(next(data_iter))
print(next(data_iter))
```

:::

Batch sizes of 1, such as observed so far, are useful for illustration. In dep learning, small batch sizes require less memory during training but lead to more noisy model updates. 

An example of using a larger batch size in the current data loader. 

```{python}
# Setting stride to 4 so each input is not overlapping (overfitting) but also not skipping any words (using whole dataset)
dataloader = create_dataloader_v1(
    raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)
data_iter = iter(dataloader)
inputs, targets = next(data_iter)
print(f'Inputs with shape {inputs.shape}\n', inputs)
print(f'\nTargets with shape {targets.shape}\n',targets)
```

## 2.7 Creating token embeddings
The last step to prepare the input text for LLM training is to convert the token IDs into embedding vectors. Initially the weights for the embeddings are random, but then are trained in the learning process. 

A continuous vector representation, or embedding, is necessary since GPT-like LLMs are deep neural networks trained with the backpropagation algorithm. 

Assume you begin with the following input token with IDs 2,3,5, and 1.

```{python}
input_ids = torch.tensor([2,3,5,1])
```
Then the vocabularly is small with only 6 words (BPE has 50,257) and create embeddings of size 3 (in GPT-3 the embedding size is 12,288 dimensions)

```{python}
vocab_size = 6
output_dim = 3
# Set seed for reproducibility
torch.manual_seed(123)
embedding_layer = torch.nn.Embedding(num_embeddings=vocab_size,embedding_dim=output_dim) # num_embeddings is number of words in vocab, embedding_dim is number of embedding dimensions]
print(f'Dimensions of embeddings {embedding_layer.weight.shape}\n',embedding_layer.weight)
```

The weight matrix is full of small, random values. Which is become optimized during LLM training. Each row is for each of the six possible tokens in the vocabularly, and there is one column for each of the three embedding dimensions. 

Now apply it to a token ID to obtain an embedding vector. 

```{python}
print(embedding_layer(torch.tensor([3])))
```

When compared to previous embedding matrix with all 6 words in the vocabularly, we can see this row corresponds to the 4th row (index 3). So essentially the embedding layer is a lookup operation that retrieves rows from the embedding layer's weight matrix via a token ID. You give it a token ID, it looks up what row to return to give a continuous value to represent it and feed into the LLM.


```{python}
print(f'Dimensions of embeddings {embedding_layer.weight.shape}\n',embedding_layer.weight)
print(embedding_layer(torch.tensor([3])))
```

::: callout-note
When looking at embeddings compared to one-hot encoding, the embedding layer approach is essentially a more efficient way of implementing one-hot encoding followed by matrix multiplication in a fully connected layer. See [this link](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/03_bonus_embedding-vs-matmul/embeddings-and-linear-layers.ipynb) for the comparison. 
:::

Extending this notion to all four input IDs

```{python}
print(embedding_layer(input_ids))
```

Now there's a continuous value representation of each token ID, there needs to be positional information implemented as well. 

## 2.8 Encoding word positions
