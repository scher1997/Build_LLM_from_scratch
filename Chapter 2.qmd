---
title: "Chapter 2 - Working with Text Data"
format: html
---

# 2 - Working with Text Data

## 2.1 Understanding word embeddings
We need to understand that LLMs cannot process raw text directly and instead need to be represented with continuous-valued vectors. The conversion is done through embeddings which take in the raw data, in the form of video, audio, text, etc. and converts them to a dense vector of continous values, which are then used in deep learning.

## 2.2 Tokenizing text
Before creating embeddings however, text data requires a preprocessing step to create tokens which is how we split input text up into pieces for an embedding layer. To summarize, we split an input text into individual tokens, which are either words or special characters, such as punctuation characters. These are later converted into token IDs (like a lookup table) which are then used to create embeddings. 
```{python}
with open('the_verdict.txt', 'r',encoding='utf_8') as f:
    raw_text = f.read()

print('Total number of characters:', len(raw_text))
print(raw_text[:99])
```

Goal is to tokenize this 20479 character short story. 

Then turn into embeddings for LLM training. 

So use regex to get a list of individual words, whitespaces, and punctuation characters
```{python}
import re

text = 'Hello, world. This, is a test.'
result = re.split(r'(\s)',text)
print(result)
```

Now split on whitespaces and commas (\s), and periods ([,.])
```{python}
result = re.split(r'([,.]|\s)',text)
print(result)
```

We could remove whitespaces so we just have words, commas, and periods
```{python}
result = [item for item in result if item.strip()]
print(result)
```

:::{.callout-note}
Potential experiment is to keep white spaces in the tokenizer and see the differences in model output. Think about how picky coding languages are when it comes to spacing!
:::

So now modify the tokenization scheme to work on other types of punction, such as question marks, quotation marks, and the double dashes shown in the first 100 characters. 

```{python}
text = 'Hello, world. Is this-- a test?'
result = re.split(r'([,.:;?_!"()\']|--|\s)',text)
result = [item.strip() for item in result if item.strip()]
print(result)
```

Now apply to the entire short story
```{python}
preprocessed = re.split(r'([,.:;?_!"()\']|--|\s)',raw_text)
preprocessed = [item.strip() for item in preprocessed if item.strip()]
print(len(preprocessed))
print(preprocessed[:30])
```

## 2.3 Converting tokens into token IDs
We now take these tokens which are a Python string and convert them to an integer representation to produce token IDs. An intermediate step before going to embedding vectors. This mapping from tokens to token IDs requires a ``vocabulary''. 

```{python}
all_words = sorted(set(preprocessed)) # Get all unique words
vocab_size = len(all_words)
print(vocab_size)
```

Let's show some of this lookup, it'll be a perfect use of a dictionary data type. 
```{python}
vocab = {token:integer for integer, token in enumerate(all_words)}
for i, item in enumerate(vocab.items()):
    print(item)
    if i > 50:
        break
```

:::{.callout-tip}
We will use an inverse of this to go from our token IDs back to the original words!
:::

Let's create a complete tokenizer class which can go both directions, `encode` method (string-to-integer) and `decode` (integer-to-string) method. 

```{python}
class SimpleTokenizerV1:
    def __init__(self, vocab):
        # Store the vocab as a class attribute for access in the encode and decode methods
        self.str_to_int = vocab
        # Create an inverse vocabularly that maps token IDs back to the original text tokens
        self.int_to_str = {i:s for s, i in vocab.items()}

    # Process input text into token IDs
    def encode(self, text):
        preprocessed = re.split(r'([,.:;?_!"()\']|--|\s)',text) # Seperate words/punctution
        preprocessed = [item.strip() for item in preprocessed if item.strip()] # Remove white space
        ids = [self.str_to_int[s] for s in preprocessed] # Input string and get key from vocab dict
        return ids

    # Convert token IDs back to text
    def decode(self, ids):
        text = " ".join([self.int_to_str[i] for i in ids])
        # Replace spaces before the specified punctuation
        text = re.sub(r'\s+([,.:;?_!"()\'])', r'\1', text)
        return text

```

* `__init__: ` This is the constructor method that initializes the class. It takes vocab as an argument.
    +  `self.str_to_int:`  Stores the vocabulary dictionary where keys are strings (tokens) and values are integers (token IDs).
    + `self.int_to_str:` Creates an inverse dictionary where keys are token IDs and values are the original strings (tokens).

* `encode:` This method processes input text into token IDs.
    + `re.split(r'([,.:;?_!"()\']|--|\s)', text):` Splits the text into words and punctuation based on the specified regular expression pattern.
    + `[item.strip() for item in preprocessed if item.strip()]:` Strips whitespace from each item and removes empty strings.
    + `[self.str_to_int[s] for s in preprocessed]:` Converts each token in the preprocessed list to its corresponding token ID using the str_to_int dictionary.
    + `return ids:` Returns the list of token IDs.

*  `decode:` This method converts token IDs back to text.
    + `" ".join([self.int_to_str[i] for i in ids]):` Joins the tokens corresponding to the token IDs into a single string with spaces in between.
    + `re.sub(r'\s+([,.:;?_!"()\'])', r'\1', text):` Removes spaces before punctuation marks.
    + `return text:` Returns the reconstructed text.

### Example 
```{python}
vocab = {'hello': 1, 'world': 2, ',': 3}
tokenizer = SimpleTokenizerV1(vocab)

encoded = tokenizer.encode("hello, world")
print(encoded)  # Output: [1, 3, 2]

decoded = tokenizer.decode(encoded)
print(decoded)  # Output: "hello, world"
```

Back to the short story vocab

```{python}
vocab = {token:integer for integer, token in enumerate(all_words)}
tokenizer = SimpleTokenizerV1(vocab)

text = """"It's the last he painted, you know," Mrs. Gisburn said with pardonable pride."""

ids = tokenizer.encode(text)
print(ids)
```

The code above prints the following token IDs followed by the code below for the decoding

```{python}
print(tokenizer.decode(ids))
```

We can apply it to other text as well but be careful if any words are not in our vocab
```{python}
text = 'Hello, do you like tea?'
print(tokenizer.encode(text))
```


## 2.4 Adding special context tokens